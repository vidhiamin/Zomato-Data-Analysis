                    Introduction to data analysis and machine learning
                               deVidhi Amin
                                                          

Load LendingClub.
```{r}
library(tidyverse)
setwd("C:/Users/Dhvani/Desktop/data analysis and machine learning/exam")
load(file = "LendingClub")
```

#load product_sale dataset.
```{r}
Product_Sales <- read_excel("Product Sales.xlsx")
View(Product_Sales)
```

1.	Do a quick Exploratory Data Analysis for the dataset Lending Club and Product Sales. Exploratory Data Analysis should show at least two graphs for each case and whatever description you think is necessary. Whenever it makes sense report descriptive statistics. 

#Analysis for Lendingclub data set.


Now, we will find out the correlation among variables in lendingdata set.
```{r}
library("corrgram")
corrgram(LendingClub, order=NULL, panel=panel.shade, text.panel=panel.txt,
           main="Correlogram")
```
Loan_amnt is highly positively correlated with the pct_loan_income.we can see that from statistical value.

```{r}
cor(LendingClub$loan_amnt, LendingClub$pct_loan_income)
```


Loan_amnt is medium positively correlated with the adjusted_annual_inc.we can see that from statistical value.
```{r}
cor(LendingClub$loan_amnt , LendingClub$adjusted_annual_inc)
```

dti(Debt to Income) is medium negatively correlated with the adjusted_annual_inc..we can see that from statistical value.
```{r}
cor(LendingClub$dti, LendingClub$adjusted_annual_inc)
```
So,in a similar way,  we can also find the relation for rest of variables.

Is there any relationship between residence_property and loan_amnt?
```{r}
 ggplot(LendingClub, aes(x = residence_property, y = loan_amnt)) + 
   geom_boxplot(outlier.color = "red", outlier.shape = 1)
```
From this plot, we can say that interquartile range of own residence_property is more compared with rent recidence_property type.Median value of own house type is little bit more compared with rent recidence_type.

Now, we will see the comparison between the loan amount and count of loan.
```{r}
library(ggplot2)
ggplot(LendingClub, aes(x = loan_amnt, fill = ..count..)) +
  geom_histogram(binwidth = 5000) +
  ggtitle("Figure 1 Histogram of loan amount") +
  ylab("Count of loan") +
  xlab("loan amount") + 
  theme(plot.title = element_text(hjust = 0.5))

```
The above plot is right sked graph. From this plot we can say that as loan amount increases, the count of loan is decreases.So, the count of loan is more for smaller value of loan amount.


Now we will see how many inquiries we received in last 6 months.
```{r}
library(ggplot2)
ggplot(LendingClub, aes(x = inq_last_6mths, fill = ..count..)) +
  geom_histogram() +
  scale_x_continuous(breaks = seq(0,6,1))+
  ggtitle("Figure 1 Histogram of inquiries for loan") +
  ylab("Count of inquiries") +
  xlab("The number of inquiries in past 6 months") + 
  theme(plot.title = element_text(hjust = 0.5))
```
From this plot, we can say that, as months increases, the number of inquiries decrease.

#Analysis for product_sales data set.

To identify the correlation among variables.
```{r}
library("corrgram")
corrgram(Product_Sales, order=NULL, panel=panel.shade, text.panel=panel.txt,
           main="Correlogram")
```
From this correlogram , we can find that there is no negative correlation among variables.

ExcelAddInItemsold and AddInRevenue is positively highly corelated with each other.Statistically we get the following value.
```{r}
cor(Product_Sales$ExcelAddInItemsSold, Product_Sales$AddInRevenue)
```

AddInSalesTransactions and TransactionsAutoPay is positively highly corelated with each other.Statistically we get the following value.
```{r}
cor(Product_Sales$AddInSalesTransactions,Product_Sales$TransactionsAutoPay)
```

CompanySize and AnalyticsSoftwareRevenue is positively medium corelated with each other.Statistically we get the following value.
```{r}
cor(Product_Sales$CompanySize, Product_Sales$AnalyticsSoftwareRevenue)
```
So, with the similar way, We can find  the correlation for rest of variables.


Is there any relaitonship among companyworldregion and companysize?
```{r}
ggplot(Product_Sales, aes(x = CompanyWorldRegion, y = CompanySize)) + 
     geom_boxplot(outlier.color = "red", outlier.shape = 1)
```

Now we will see distrubution of company size.
```{r}
i<-ggplot(Product_Sales) + 
  geom_bar(aes(x = CompanySize), color="gold", fill="darkorchid2") 
i
```
It is right skwed graph. From this plot we can say that, there are more number of small company. when company size is increasing then number of company is decreasing.

Is there any relationship between AddInRevenue and ExcelAddInItemsSold?
```{r}
library(ggplot2)
a<- ggplot(Product_Sales, aes(x=Product_Sales$ExcelAddInItemsSold, y=Product_Sales$AddInRevenue)) + 
stat_smooth(method = "loess", colour = "red", size = 1) +
   xlab("ExcelAddInItemsSold") + ylab("AddInRevenue") 
a
```
From this plot we can say that, these two variables are highly correlated with each other. they have linear relationship.As ExcelAddItemSold increases AddInRevenue is also increases.

Is there any relationship among AnalyticsSoftwareRevenue and companySize?
```{r}
a<- ggplot(Product_Sales, aes(x=Product_Sales$CompanySize, y=Product_Sales$AnalyticsSoftwareRevenue)) + 
stat_smooth(method = "loess", colour = "red", size = 1) +
   xlab("CompanySize") + ylab("AnalyticsSoftwareRevenue") 
a
```
From this plot we can say that as company size increases, analyticsSoftwarerevenue also increases.

Is there any relaitonship among TransactionsAutoPay and AnalyticsSoftwareRevenue?
```{r}
a<- ggplot(Product_Sales, aes(y=Product_Sales$TransactionsAutoPay, x=Product_Sales$AnalyticsSoftwareRevenue)) + 
stat_smooth(method = "loess", colour = "red", size = 1) +
   ylab("TransactionsAutoPay") + xlab("AnalyticsSoftwareRevenue") 
a
```
From this plot, we can say that as analyticsSoftwareRevenue increases number of transactionautopay also increases but after that it is decreases.



3.	Describe PCA. Explain why would you use PCA? Choose any of the above dataset and do a PCA. Discuss, whether PCA provides you with any insight. 
[20]

#PCA

```{r}
# For PCA, first we have to check  whether all the attributes are continuous variables or not? 
str(Product_Sales)
#In this dataset, CompanyWorldRegion is a character, so we have to remove this attribute,so that we can get all continuous variables.
P2<- Product_Sales[,-3]
P2
```

```{r}
#After removing the column, we Perform a principal components analysis on the given data matrix and return the results as an object of class prcomp.
pca<-princomp(P2,cor=TRUE,score=TRUE)

#Summary of PCA
summary(pca1)

#plot
plot(pca)
```

```{r}
#Now we will plot scatter plot so it explains the variance.
plot(pca,type="l")

```


```{r}
#biplot
biplot(pca,scale=0)
```

```{r}
#to measure the principle component loading
print(pca$loadings)
```

```{r}
#Compute sd
stddev<-pca$sdev
stddev
```


```{r}
#Compute variance
variance<-stddev^2

#Explaining the proportion of variance
propvar<-variance*100/sum(variance)
propvar
```



```{r}
#to get to know how many components to select for modelling stage .
plot(propvar,xlab="Principal Component",ylab="Prop of Variance Explained",type="b")
```

```{r}
plot(cumsum(propvar),xlab="Principal Component",ylab="Cumulative Prop of Variance Explained",type="b")
```

4.	Build a neural network model for the dataset Lending Club. Test your model and report confusion matrix. 
```{r}
#convert all variables to numeric and remove residence_property column because neural network works very well for numeric value not char.

#Here I only took  observation to run neural network because in larger data set neural network took too much time to run.

library("tidyverse")

#here I selected all columns expect residency_property
LendingClub <- LendingClub%>% select(loan_default, loan_amnt, adjusted_annual_inc, pct_loan_income , dti , months_since_first_credit, inq_last_6mths , open_acc , bc_util , num_accts_ever_120_pd , pub_rec_bankruptcies)

#convert all variables into numeric
LendingClub %>% mutate_if(is.factor, as.numeric) -> LendingClub

#structure of data set.
str(LendingClub)	


```


```{r}
#This will give min and max value of each variable.
apply(LendingClub, 2, range)
```
Seems like scale of each variable is not same.

```{r}
#normalizing the data in interval [0,1]
#normalization is necessary so that each variable is scale properly and none of variable is overdominates.
#scale function will give mean=0 and standard deviation = 1 for each variable.
MAXVAUE<- apply(LendingClub, 2, max)
MINVALUE<- apply(LendingClub, 2, min)

```


```{r}
LendingClub<- as.data.frame(scale(LendingClub, center = MINVALUE , scale = MAXVAUE-MINVALUE))
```


```{r}
#let's create train and test dataset
LendingClub<- sample_n(LendingClub, 1000)
ind1 <- sample(1:nrow(LendingClub) ,700)
trainDF <- LendingClub[ind1,]
testDF<- LendingClub[-ind1 , ]
```


```{r}
#lets create some configration for neural network.
#say 10-4-2-1
#number of hidden layer= 2
library(neuralnet)
allvar<- colnames(LendingClub)
predictorvar<- allvar[!allvar%in%"loan_default"]
predictorvar<- paste(predictorvar, collapse = "+")
formU <- as.formula(paste("loan_default~", predictorvar, collapse = "+"))

neuralModel <- neuralnet(formula = formU, hidden= c(4,2) , linear.output=T , data= trainDF, stepmax = 1e6)
```


```{r}
#plot neural model
plot(neuralModel)
```


```{r}
#predict for test data set
predictions<- compute(neuralModel, testDF[ ,1:10 ])
str(predictions)
```

```{r}
predictions <- predictions$net.result*(max(testDF$loan_default) - min(testDF$loan_default)) + min(testDF$loan_default)

actual1 <- (testDF$loan_default)* (max(testDF$loan_default) - min(testDF$loan_default))+ min(testDF$loan_default)
```

```{r}
#confusion matrix
cm<-table(round(predictions) , actual1)
cm
```

```{r}
#Accuracy of model:
Accuracy <- (cm[1,1] + cm[2,2])/(cm[1,2] + cm[1,1] + cm[2,1] +cm[2,2])

Accuracy<- Accuracy*100
Accuracy

```



5.	Build a decision tree model for the dataset Lending Club. Test your model and report confusion matrix. 
[20]

```{r}
#here i created dataset in which there are 500 rows for loan default value is 0 and 1. because in the original dataset, distrubution of loan_default is not equal.so decision tree does not divide the data in a proper way. so, I came up with the following solution.

#load data set.
library(tidyverse)
setwd("C:/Users/Dhvani/Desktop/data analysis and machine learning/exam")
load(file = "LendingClub")

LendingClub1 <- LendingClub %>% filter(LendingClub$loan_default==0)
LendingClub2 <- LendingClub %>% filter(LendingClub$loan_default==1)
LendingClub1<- sample_n(LendingClub1, 1000)
LendingClub2<- sample_n(LendingClub2, 1000)
final1 <- rbind(LendingClub1, LendingClub2)

#load library
library(dplyr)
library(rpart)
library(rpart.plot)

#next we look distrubution of loan_default.
table(final1$loan_default)

```
From this output we can say that ~12.5% of the data is present

```{r}
#train and test data set

ind2 <- sample(1:nrow(final1),1800)
trainDF1 <- final1[ind2,]
testDF1<- final1[-ind2 , ]

#Classification tree
fit1 <- rpart(loan_default ~. , method = "class",   data = trainDF1)

#Next we look at the results
print(fit1)
```


```{r}
#Decision tree
rpart.plot(fit1)
```


```{r}
#confusion matrix and prune the tree

pr.fit1<- prune(fit1, cp = fit1$cptable[which.min(fit1$cptable[,"xerror"]),"CP"])
rpart.plot(pr.fit1)

pred1 <- predict(pr.fit1, type="class")
(conf.mat <- table(pred1,trainDF1$loan_default))	    

```



```{r}
#Accuracy of model:
accuracy <- (conf.mat[1,1] + conf.mat[2,2]) / (conf.mat[1,1] + conf.mat[2,2] + conf.mat[1,2] + conf.mat[2,1])


Accuracy<- accuracy*100
Accuracy
```










                                                          